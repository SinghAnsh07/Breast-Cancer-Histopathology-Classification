{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIzttGdntUqTEvI1yaWBC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SinghAnsh07/DataScience/blob/main/breast%20cancer%20detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq91uRU2W9fw",
        "outputId": "deb45de0-0828-4418-887a-19c747a21e30"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -oq \"/content/drive/MyDrive/BreaKHis_v1.zip\" -d /content/\n",
        "!ls /content/BreaKHis_v1/histology_slides/breast\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcAHte-ZW988",
        "outputId": "b362c0ea-437a-4391-9a55-ebaa48cc156d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "benign\tcount_files.sh\tmalignant  README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm -q\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import timm\n",
        "from datetime import datetime\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "data_root = \"/content/BreaKHis_v1/histology_slides/breast\"\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "learning_rate = 3e-4\n",
        "\n",
        "magnifications = [\"40X\", \"100X\", \"200X\", \"400X\"]\n",
        "all_results = {}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evy60GXjYOq8",
        "outputId": "cd80d65e-5216-432f-b168-2ed6fbe5bc62"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BCDataset(Dataset):\n",
        "    def __init__(self, files, labels, transform=None):\n",
        "        self.files = files\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.5),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "PxmOsFokYPCR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(magnification):\n",
        "    benign = os.path.join(data_root, \"benign\")\n",
        "    malignant = os.path.join(data_root, \"malignant\")\n",
        "\n",
        "    files, labels = [], []\n",
        "\n",
        "    for path, label in [(benign, 0), (malignant, 1)]:\n",
        "        for root, _, names in os.walk(path):\n",
        "            if os.path.basename(root) == magnification:\n",
        "                for n in names:\n",
        "                    if n.lower().endswith(\".png\"):\n",
        "                        files.append(os.path.join(root, n))\n",
        "                        labels.append(label)\n",
        "\n",
        "    if len(files) == 0:\n",
        "        print(f\"⚠️ No images for {magnification}\")\n",
        "        return None, None, 0, 0, 0\n",
        "\n",
        "    data = list(zip(files, labels))\n",
        "    random.shuffle(data)\n",
        "    files, labels = zip(*data)\n",
        "\n",
        "    split = int(0.8 * len(files))\n",
        "    train_files, val_files = files[:split], files[split:]\n",
        "    train_labels, val_labels = labels[:split], labels[split:]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        BCDataset(train_files, train_labels, train_transform),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        BCDataset(val_files, val_labels, val_transform),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, len(files), labels.count(0), labels.count(1)\n"
      ],
      "metadata": {
        "id": "w8OFg-WXYPUj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "\n",
        "for mag in magnifications:\n",
        "    acc, _ = train_magnification(mag)\n",
        "    all_results[mag] = acc\n",
        "\n",
        "print(\"\\nSUMMARY\")\n",
        "for mag, acc in all_results.items():\n",
        "    if acc > 0:\n",
        "        print(f\"{mag}: {acc*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"{mag}: SKIPPED\")\n",
        "\n",
        "print(\"Time:\", datetime.now() - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXUS0a3AYPvp",
        "outputId": "b66402b4-9d1a-41f0-e9a9-352c4d90da22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training 40X\n",
            "⚠️ No images for 40X\n",
            "\n",
            "Training 100X\n",
            "⚠️ No images for 100X\n",
            "\n",
            "Training 200X\n",
            "⚠️ No images for 200X\n",
            "\n",
            "Training 400X\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400X Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
            "400X Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "400X Epoch 3: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
            "400X Epoch 4: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "400X Epoch 5: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
            "400X Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "400X Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "400X Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
            "400X Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
            "400X Epoch 10: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "400X Epoch 11: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
            "400X Epoch 12: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
            "400X Epoch 13: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
            "400X Epoch 14: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
            "400X Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
            "400X Epoch 16: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
            "400X Epoch 17: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
            "400X Epoch 18: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "400X Epoch 19: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "400X Epoch 20: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
            "400X Epoch 21: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "400X Epoch 22: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
            "400X Epoch 23: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
            "400X Epoch 24: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
            "400X Epoch 25: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "400X Epoch 26: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
            "400X Epoch 27: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
            "400X Epoch 28: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
            "400X Epoch 29: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "400X Epoch 30: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
            "400X Epoch 31: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "400X Epoch 32: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
            "400X Epoch 33: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
            "400X Epoch 34: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "400X Epoch 35: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
            "400X Epoch 36: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "400X Epoch 37: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "400X Epoch 38: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "400X Epoch 39: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
            "400X Epoch 40: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
            "400X Epoch 41: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "400X Epoch 42: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "400X Epoch 43: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "400X Epoch 44: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
            "400X Epoch 45: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
            "400X Epoch 46: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "400X Epoch 47: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "400X Epoch 48: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
            "400X Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
            "400X Epoch 50: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SUMMARY\n",
            "40X: SKIPPED\n",
            "100X: SKIPPED\n",
            "200X: SKIPPED\n",
            "400X: 100.00%\n",
            "Time: 0:00:48.070262\n"
          ]
        }
      ]
    }
  ]
}